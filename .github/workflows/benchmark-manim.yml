name: "Manim Benchmark"

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
jobs:
  benchmark:
    runs-on: "ubuntu-20.04"
    env:
      DISPLAY: :0
      PYTEST_ADDOPTS: "--color=yes" # colors in pytest
    steps:
      - name: Checkout the repository
        uses: actions/checkout@v3

      - name: Install Poetry
        run: |
          pipx install poetry
          poetry config virtualenvs.prefer-active-python true

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "poetry"

      - name: Setup cache variables
        shell: bash
        id: cache-vars
        run: |
          echo "date=$(/bin/date -u "+%m%w%Y")" >> $GITHUB_OUTPUT

      - name: Install and cache ffmpeg (all OS)
        uses: FedericoCarboni/setup-ffmpeg@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
        id: setup-ffmpeg

      - name: Install system dependencies (Linux)
        if: runner.os == 'Linux'
        uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: python3-opengl libpango1.0-dev xvfb
          version: 1.0

      - name: Install Texlive (Linux)
        if: runner.os == 'Linux'
        uses: teatimeguest/setup-texlive-action@v2
        with:
          cache: true
          packages: scheme-basic fontspec inputenc fontenc tipa mathrsfs calligra xcolor standalone preview doublestroke ms everysel setspace rsfs relsize ragged2e fundus-calligra microtype wasysym physics dvisvgm jknapltx wasy cm-super babel-english gnu-freefont mathastext cbfonts-fd

      - name: Start virtual display (Linux)
        if: runner.os == 'Linux'
        run: |
          # start xvfb in background
          sudo /usr/bin/Xvfb $DISPLAY -screen 0 1280x1024x24 &

      - name: Install manim
        run: |
          poetry config experimental.new-installer false
          poetry install

      - name: Run Benchmarks
        run: |
          poetry run pytest --dist no -n 0 --benchmark-only --benchmark-json output.json --benchmark-warmup=off tests/benchmarks/

      - name: Download previous benchmark data
        uses: actions/cache@v3
        with:
          path: ./cache
          key: ${{ runner.os }}-benchmark

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: "Manim Benchmark"
          gh-pages-branch: "gh-bech-pages"
          benchmark-data-dir-path: "dev/bench"
          github-token: "${{secrets.GITHUB_TOKEN}}"
          auto-push: ${{github.event_name!='pull_request'}}
          tool: "pytest"
          output-file-path: output.json
          external-data-json-path: ./cache/benchmark-data.json
          fail-on-alert: true
          comment-always: true
          comment-on-alert: true
          max-items-in-chart: 10
